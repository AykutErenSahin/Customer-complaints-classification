{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install zemberek-python          #gerekli yüklemeler\n",
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "id": "sGONmJlBwuqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW5JQfhqoCdm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from snowballstemmer import TurkishStemmer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('turkish'))\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from zemberek import TurkishSpellChecker, TurkishMorphology, TurkishTokenizer, TurkishSentenceNormalizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "morphology = TurkishMorphology.create_with_defaults()\n",
        "normalizer = TurkishSentenceNormalizer(morphology)\n",
        "extractor = TurkishSentenceExtractor()\n",
        "\n",
        "def preprocess_with_zemberek(text):\n",
        "    stop_words = set(stopwords.words('turkish'))\n",
        "\n",
        "    # Temizleme\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)        # Noktalama işaretlerini kaldır\n",
        "    text = re.sub(r'\\d+', '', text)            # Sayıları kaldır\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()   # Birden fazla boşlukları tek boşlukla değiştir\n",
        "\n",
        "    # Cümlelere ayırma\n",
        "    sentences = extractor.from_paragraph(text)\n",
        "    processed_words = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Normalizasyon\n",
        "        normalized = normalizer.normalize(sentence)\n",
        "        tokens = normalized.split()\n",
        "\n",
        "        for word in tokens:\n",
        "            # Stop words kontrolü\n",
        "            if word not in stop_words:\n",
        "                # Stemming\n",
        "                analysis = morphology.analyze(word)\n",
        "                if analysis.analysis_results:\n",
        "                    word = str(analysis.analysis_results[0].item.lemma)\n",
        "                processed_words.append(word)\n",
        "\n",
        "    return ' '.join(processed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyXve5qj66di",
        "outputId": "a4d13000-c589-47eb-afc3-31a27380d4cd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:zemberek.morphology.turkish_morphology:TurkishMorphology instance initialized in 30.078245639801025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-14 14:17:19,631 - zemberek.morphology.turkish_morphology - INFO\n",
            "Msg: TurkishMorphology instance initialized in 30.078245639801025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict(input_file_link, output_file_link):\n",
        "\n",
        "    data = pd.read_csv(input_file_link, sep=';', encoding='cp1254', on_bad_lines='skip')          #türkçe encoding\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    pred_data = pd.read_csv(output_file_link, sep=';', encoding='utf-8', on_bad_lines='skip')          #türkçe encoding\n",
        "    pred_df = pd.DataFrame(pred_data)\n",
        "\n",
        "    df['Etiket'] = pd.factorize(df['Kategori'], sort=True)[0]             #kategorilere sayı ataması yapıyoruz\n",
        "    etiket_to_kategori = dict(enumerate(df['Kategori'].unique()))\n",
        "    df['islenmis_Yorum'] = df['Yorum'].apply(preprocess_with_zemberek)\n",
        "\n",
        "    X = df['islenmis_Yorum']\n",
        "    y = df['Etiket']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
        "\n",
        "    pipeSVC = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 2), max_features=5000)) , ('clf', LinearSVC())])\n",
        "\n",
        "    pipeSVC.fit(X_train, y_train)\n",
        "    predictSVC = pipeSVC.predict(X_test)\n",
        "    print(\"SVC : {:.2f}\".format(accuracy_score(y_test, predictSVC)))\n",
        "\n",
        "    pred_df['islenmis_Yorum'] = pred_df['Yorum'].apply(preprocess_with_zemberek)\n",
        "    tahmin_etiketler = pipeSVC.predict(pred_df['islenmis_Yorum'])\n",
        "    pred_df['Tahmin_Kategori'] = [etiket_to_kategori[etiket] for etiket in tahmin_etiketler]\n",
        "\n",
        "    pred_df.to_csv('tahmin_sonucu.csv', index=False)"
      ],
      "metadata": {
        "id": "SsrMnpGf1K-a"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = input(\"Lütfen eğitim CSV dosyasının yolunu girin: \")\n",
        "output_file = input(\"Lütfen tahmin edilecek CSV dosyasının yolunu girin: \")\n",
        "\n",
        "train_and_predict(input_file,output_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1CcWNda8zLe",
        "outputId": "3b79fb69-b1b8-4e6d-ff6c-d149ef08165b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lütfen eğitim CSV dosyasının yolunu girin: https://raw.githubusercontent.com/AykutErenSahin/Customer-complaints-classification/main/atm.csv\n",
            "Lütfen tahmin edilecek CSV dosyasının yolunu girin: https://raw.githubusercontent.com/AykutErenSahin/Customer-complaints-classification/main/ATM_test.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC : 0.80\n",
            "SVC : 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "https://raw.githubusercontent.com/AykutErenSahin/Customer-complaints-classification/main/atm.csv\n",
        "https://raw.githubusercontent.com/AykutErenSahin/Customer-complaints-classification/main/ATM_test.csv"
      ],
      "metadata": {
        "id": "QVc5WyprwZmf"
      },
      "execution_count": 58,
      "outputs": []
    }
  ]
}